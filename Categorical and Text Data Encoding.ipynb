{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245097ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.conda/envs/akashnew/lib/python3.9/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.conda/envs/akashnew/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/student/.conda/envs/akashnew/lib/python3.9/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.conda/envs/akashnew/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.0.1 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d64775f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b324d3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "y = data.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3711eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it categorical\n",
    "y = pd.Categorical.from_codes(y,data.target_names)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affd158d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'setosa', 'setosa', 'setosa', 'setosa', ..., 'virginica', 'virginica', 'virginica', 'virginica', 'virginica']\n",
       "Length: 150\n",
       "Categories (3, object): ['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66daeae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply label encoding\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a762be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a6fc6",
   "metadata": {},
   "source": [
    "# 2. Onehot encoding technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0316d7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "y = data.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41ff08ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'setosa', 'setosa', 'setosa', 'setosa', ..., 'virginica', 'virginica', 'virginica', 'virginica', 'virginica']\n",
       "Length: 150\n",
       "Categories (3, object): ['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make it categorical\n",
    "y = pd.Categorical.from_codes(y,data.target_names)        \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28bfb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply \n",
    "enc = OneHotEncoder()\n",
    "y_encoded = enc.fit_transform(y.reshape(-1,1))\n",
    "y_encoded = y_encoded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba079434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaea3cc",
   "metadata": {},
   "source": [
    " # word encoding technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3062e86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m452.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m484.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m465.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/student/.local/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.5.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 kB\u001b[0m \u001b[31m493.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m490.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m476.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.5.5 tqdm-4.65.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install gensim\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c681ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2a06388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25661c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b52c6ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13a20b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text to be preprocessed and embedded\n",
    "text = \"Hi everyone.my name is Akash.I am studying in nmamit in third semester.Nice to meet you my friend , have a good day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6924db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the word into individual words\n",
    "words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4456802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the word by converting it into lowercase and removing punctuation\n",
    "processed_words = [word.lower() for word in words if word.isalpha() and word.lower() not in nltk.corpus.stopwords.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1347e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding using the Word2Vec algorithm for gensim\n",
    "model = Word2Vec([processed_words],vector_size=100,window=5,min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bdccc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studying', 'nmamit', 'meet', 'friend', 'day']\n"
     ]
    }
   ],
   "source": [
    "print(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5841f41d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'akash' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get the embedding vector for a specific word\u001b[39;00m\n\u001b[1;32m      2\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124makash\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m embedding_vector \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/akashnew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/.conda/envs/akashnew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/.conda/envs/akashnew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'akash' not present\""
     ]
    }
   ],
   "source": [
    "# get the embedding vector for a specific word\n",
    "word = \"akash\"\n",
    "embedding_vector = model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d2e1c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
      "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
      "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
      "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
      "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
      "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
      " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
      "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
      " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
      "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
      "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
      "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
      " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
      " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
      "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
      "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
      "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
      " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
      " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
      " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
      " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
      " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
      " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
      " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
      "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbee1a2",
   "metadata": {},
   "source": [
    "# new program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f96d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f75ea63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6889b737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author   \n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus  \\\n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data =pd.read_csv('fake_news.csv')\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cda88153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5cf85ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd0dc98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacing the misiing values with empty string\n",
    "news_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0e1e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the author name and the news title\n",
    "news_data['content'] = news_data['author'] + ' ' +news_data['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ca9338c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>Darrell Lucus House Dem Aide: We Didn’t Even S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Consortiumnews.com Why the Truth Might Get You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>Jessica Purkiss 15 Civilians Killed In Single ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Howard Portnoy Iranian woman jailed for fictio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author   \n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus  \\\n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label   \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \\\n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "\n",
       "                                             content  \n",
       "0  Darrell Lucus House Dem Aide: We Didn’t Even S...  \n",
       "1  Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...  \n",
       "2  Consortiumnews.com Why the Truth Might Get You...  \n",
       "3  Jessica Purkiss 15 Civilians Killed In Single ...  \n",
       "4  Howard Portnoy Iranian woman jailed for fictio...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first 5 rows rows of the dataset\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7e6aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = news_data.drop(columns='label',axis='1')\n",
    "y = news_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a2f7b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1df5d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [porter_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ''.join(stemmed_content)\n",
    "    return (stemmed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4581ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Darrell Lucus House Dem Aide: We Didn’t Even S...\n",
      "1        Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...\n",
      "2        Consortiumnews.com Why the Truth Might Get You...\n",
      "3        Jessica Purkiss 15 Civilians Killed In Single ...\n",
      "4        Howard Portnoy Iranian woman jailed for fictio...\n",
      "                               ...                        \n",
      "20795    Jerome Hudson Rapper T.I.: Trump a ’Poster Chi...\n",
      "20796    Benjamin Hoffman N.F.L. Playoffs: Schedule, Ma...\n",
      "20797    Michael J. de la Merced and Rachel Abrams Macy...\n",
      "20798    Alex Ansary NATO, Russia To Hold Parallel Exer...\n",
      "20799              David Swanson What Keeps the F-35 Alive\n",
      "Name: content, Length: 20800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(news_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3fee7715",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = news_data['content'].values\n",
    "y = news_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9244b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Darrell Lucus House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It'\n",
      " 'Daniel J. Flynn FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'\n",
      " 'Consortiumnews.com Why the Truth Might Get You Fired' ...\n",
      " 'Michael J. de la Merced and Rachel Abrams Macy’s Is Said to Receive Takeover Approach by Hudson’s Bay - The New York Times'\n",
      " 'Alex Ansary NATO, Russia To Hold Parallel Exercises In Balkans'\n",
      " 'David Swanson What Keeps the F-35 Alive']\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df8e1f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "33200673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the textual data to feature vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bffe73b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 23355)\t0.18006497451107856\n",
      "  (0, 22649)\t0.26575278886038384\n",
      "  (0, 22289)\t0.3484071341454308\n",
      "  (0, 19171)\t0.22537992364975484\n",
      "  (0, 12902)\t0.3024224900242886\n",
      "  (0, 12528)\t0.24883399099107747\n",
      "  (0, 11409)\t0.20615188166061463\n",
      "  (0, 11307)\t0.1532265401605094\n",
      "  (0, 10387)\t0.1844880289323935\n",
      "  (0, 7574)\t0.23047267305353566\n",
      "  (0, 6145)\t0.24677171892553343\n",
      "  (0, 5800)\t0.2502787762405247\n",
      "  (0, 5508)\t0.2993429551929777\n",
      "  (0, 4507)\t0.20531415441295317\n",
      "  (0, 3862)\t0.30579573877221844\n",
      "  (0, 904)\t0.26354004814013343\n",
      "  (1, 23748)\t0.2966210296019264\n",
      "  (1, 15149)\t0.15862263711495958\n",
      "  (1, 10134)\t0.18787145765749733\n",
      "  (1, 8420)\t0.7045992054867243\n",
      "  (1, 5469)\t0.2624012615566619\n",
      "  (1, 4298)\t0.19024289659874757\n",
      "  (1, 3509)\t0.37751839443307017\n",
      "  (1, 3075)\t0.15310531118537438\n",
      "  (1, 2544)\t0.2899843833664323\n",
      "  :\t:\n",
      "  (20797, 11262)\t0.12516633187998083\n",
      "  (20797, 10427)\t0.20792477683235197\n",
      "  (20797, 5559)\t0.20389975589596085\n",
      "  (20797, 3394)\t0.14456424605079038\n",
      "  (20797, 2273)\t0.31989436828531154\n",
      "  (20797, 1472)\t0.29969673985755974\n",
      "  (20797, 1233)\t0.0987242947097849\n",
      "  (20797, 544)\t0.28338371263237516\n",
      "  (20798, 21775)\t0.11011861411023321\n",
      "  (20798, 18626)\t0.21201851100491342\n",
      "  (20798, 15626)\t0.43299674985280595\n",
      "  (20798, 14500)\t0.3026696913367681\n",
      "  (20798, 10764)\t0.12377674027536936\n",
      "  (20798, 10228)\t0.33603181710149316\n",
      "  (20798, 7676)\t0.43299674985280595\n",
      "  (20798, 2068)\t0.43299674985280595\n",
      "  (20798, 1324)\t0.2955941555358824\n",
      "  (20798, 1009)\t0.2706299600743188\n",
      "  (20799, 23493)\t0.2683870404159613\n",
      "  (20799, 21564)\t0.10106058584391787\n",
      "  (20799, 21101)\t0.4480459367054237\n",
      "  (20799, 11815)\t0.45575108674851145\n",
      "  (20799, 5537)\t0.2993058137514979\n",
      "  (20799, 1043)\t0.4480459367054237\n",
      "  (20799, 270)\t0.4679442365402834\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51afc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
